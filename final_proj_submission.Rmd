---
title: "Adult Autism Spectrum Disorder Screening with ML"
author: "Ryan Mills & J. Carlos Garcia"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_notebook:
    toc: true
    toc_float: true
    number_sections: true
---



## Background 

### Libraries
Libraries used in Project
```{r, echo=TRUE, results='hide'}
library(foreign)
library(dplyr)
library(caret)
library(glmnet)
library(mltools)
library(data.table)
library(keras)
library(tfruns)
library(iml)
library(mlr)
library(fairness)
```

### Prefixes

**General Prefixes**

| Prefix  | Meaning               |
|:--------|:----------------------|
| `f.*`   | Functions              |
| `model.*` | Models               |
| `p.*`   | Predictions            |
| `t.*`   | Temporary Variables    |


**Model-Specific Prefixes**

| Prefix   | Associated Model              |
|:---------|:-------------------------------|
| `risk.*` | Benchmark Model                |
| `lasso.*` | Lasso Linear Model             |
| `ridge.*` | Ridge Linear Model             |
| `enet.*`  | Elastic Net Linear Model       |
| `rf.*`    | Random Forest                  |
| `gbm.*`   | Gradient Boosted Machine       |
| `svm.*`   | Support Vector Machine (SVM)   |
| `cnn.*`   | Neural Network (CNN)           |
| `fair.*`  | Fairness Analysis Section      |


## Data Analysis and Exploration 
Performed by: J. Carlos Garcia

### Data Exploration 

#### Dataset

We begin by importing our data. 
```{r}
data <- read.arff("Autism-Adult-Data.arff")
```

Our dataset is in the arff format, requiring us to use the read.arff() function out of the foreign package. This file format is very convenient as it clearly defines attributes, realtions, and data in the file - allowing us to not have to perform as.factor() or as.numeric functions manually to begin with. 

#### Summary
```{r}
summary(data)
```

Taking a look at the summary of the dataset, we find A1-A10 binary scores, these are questions on AQ-10 questionnaire used for determining if adults should be referred to take an autism assessment. A 1 or 0 indicates a point in this questionaire which are tallied. These questionnaire features are not a true indicator of autism spectrum disorder (ASD) as these only dictate who is referred and who is not for a true diagnosis test, as highlighted in the result feature. 

Our dataset has several demographic features in addition to the screening questionaire features such as age, gender, ethnicity, country of residence, and a few background features such as born with jandice, a family member had a previous diagnosis (titled "autism" on the columns), 

Additionally, we have our outcome variable Outcome which signifies if a candidate had a positive or negative outcome for ASD. 

Lastly, we can also find that there is an imbalance in the outcome variable, this mirrors the current understanding of ASD distribution amongst a general population, 1 in 31. 

It is also worth noting that both our variables were imported as intended due to the file type. Therefore, we do not need to factorize our features. 

#### Missing Values

Before the data preparation, and even analysis, we must look to see if there are any substantial missing values and examine the nature of the missing values as this will shape up our strategy for imputation later in the preparation stage. 

```{r}
t=colSums(is.na(data))
cat("Columns with Missing Values:\n")
t[t>0]

k <- round((t/nrow(data))*100, 2)
cat("\nPercentage of Missing Values in Identified Columns:\n")
k[k>0]
```
We find that there is a total of three columns with missing values, with age being the one that seems to be of random nature as opposed to systemic. Additionally, it is interesting to see that both ethnicity and relation both have the same number of missing columns. 

Let's explore this avenue:
```{r}
t=is.na(data$ethnicity)
k=is.na(data$relation)
which(t != k)
```

We find that the ethnicity and relation features are both missing values in the same entries, this confirms our initial thoughts on these values being systemic. 

We will use a method combination to indicate a missing value in these features later on in imputation for these two. 

#### Data Parity 

As discussed previously, we noted that the outcome variable has an imbalance that mirrors what would be expected in the real-world, with limited positive outcomes. 

However, there are several, multi-level variables such as age, gender, ethnicity, jundice, country of residence, relation, and used_app_before. We explore these in details to see if there is a need to combine variables, as too many levels, with under-representation, may harm our future model performance. 

As we saw in the orignal summary, I noted a peculiar instance where age had a value in the 300s. This is not likely, and it is possible that the actual value is 83. Additionally, we also see that once we reach an age of above 45, there are less and less candidates in these fields. 

We will create 5 age buckets, as is common practice for adult studies, to avoid complex and over-fitting models in later steps of this project. 

```{r}
summary(data$age)
table(data$age)
```

Next, we look at our categorical variables in closer detail to determine their balance. This information will also be very useful in data preparations. 

```{r}
cat("\nGender\n")
table(data$gender)

cat("\nEthnicity\n")
table(data$ethnicity)

cat("\nJundice\n")
table(data$jundice)

cat("\nRelation\n")
table(data$relation)

cat("\nCountry of Residency\n")
table(data$contry_of_res)

cat("\nPrevious Screening\n")
table(data$used_app_before)
```

We find a few interesting patterns in our data's various feature levels. Firstly, we find balance in the gender feature. 

Secondly, we find that there are various rare occurences in our "deeper" features, with some only having 1 entry. Additionally, we also find that the previous screening variable only has 12 inputs with a yes outcome compared to a "no" with more of the 98% makeup of the feature's distribution. 

With this in mind, we begin with our data analysis. 

### Data Analysis

#### Feature Optimization
Before we dive into data analysis, we first remove the "age_desc" variable from our data. As seen below, every single entry in our table is TRUE (or 1). Additionally, as this dataset is for "adults only" this variable was implied - therefore, it serves no value for our current uses - it was possibly left in from a previous data partitioning effort by the owners of the dataset. 
```{r}
data <- data %>%
  select(-age_desc)
```


Additionally, for simplicity, we will rename the Outcome to Outcome. The slash evoked strange behavior in our coding. 

```{r}
data$Outcome <- data$`Class/ASD`

data <- data %>%
  select(-`Class/ASD`)
```

As previously noted regarding age, we convert these to buckets and make note of the new distribution. Even with the massive outlier, 383, this value still has representation and we did not have to make an assumption about its true placement. 

```{r}
# Age Buckets
data$age <- cut(data$age,
                breaks = c(17, 24, 34, 44, Inf),
                labels = c("17-24", "25-34", "35-44", "45+"),
                right = TRUE, 
                include.lowest = TRUE)

table(data$age)
```

As we noted in the exploratory phase, ethnicity, relation, and country of residence both had various sparse levels that may not be a good representation or may not provide much value to our models down the road. 

To counteract these shortcomings, and to avoid feeding noise to our model(s) that may worsen performance and increase the chances of overfitting, we group simplify by combining sparse levels into an "other" category, keeping the big makeup of the data. 

We begin with ethnicity, we merge the latino, hispanic, pasifika, turkish, and the other levels into one for simplicity.

```{r}
# Rare Categories
t <- c("others", "Others","Latino", "Hispanic", "Pasifika", "Turkish")

# Prep
data$ethnicity <- as.character(data$ethnicity)

# Convert Rare Categories to Others
data$ethnicity <- ifelse(data$ethnicity %in% t, "Others", data$ethnicity)

# Re-Factor
data$ethnicity <- factor(data$ethnicity)

# Confirmation
table(data$ethnicity)

```

Next, we move to optimize our relation levels.

```{r}
# Combination Categories
t.family <- c("Parent", "Relative")
t.other <- c("Health care professional", "Others")

# Prep
data$relation <- as.character(data$relation)

# Update
data$relation[data$relation %in% t.family] <- "Family"
data$relation[data$relation %in% t.other]  <- "Other"
data$relation[data$relation == "Self"]     <- "Self"

# To Factor
data$relation <- factor(data$relation)

# Confirmation
table(data$relation)

```

Next, we move to optimize our country of residence and combine the rarer levels. 

```{r}

# Non-Rare Countries
t <- c("United States", "United Arab Emirates", "New Zealand",
                   "India", "United Kingdom", "Jordan", "Australia")

# Prep
data$contry_of_res <- as.character(data$contry_of_res)

# Convert Rare to Other
data$contry_of_res <- ifelse(data$contry_of_res %in% t,
                             data$contry_of_res, 
                             "Other")

# Re-Factor
data$contry_of_res <- factor(data$contry_of_res)

# Confirmation
table(data$contry_of_res)

```

#### Statistical Analysis

Below are functions that I previously used in another project in this course, these functions apply the appropriate test depending on the type of independent variable. 

```{r}
#NOTE: All helper functions were previously used in assignment 3

f.examine.numeric <- function(dependent, independent, x.name, y.name){
  # Presentation
  t.name <- paste("BoxPlot of", x.name, "vs.", y.name)
  
  # Plot
  boxplot(independent ~ dependent, main = t.name)

  # Split groups
  f <- independent[dependent == "false"]
  t <- independent[dependent == "true"]
  
  # Shapiro Length Requirements
  t.f <- if (length(f) >= 3 && length(f) <= 5000) shapiro.test(f)$p.value >= 0.05 else TRUE
  t.t <- if (length(t) >= 3 && length(t) <= 5000) shapiro.test(t)$p.value >= 0.05 else TRUE

  # Parametric Check
  if (t.f && t.t) {
    k <- t.test(independent ~ dependent)
  } else {
    k <- kruskal.test(independent ~ dependent)
  }

  association <- k$p.value < 0.01
  return(association)
}



f.examine.categorical <- function(dependent, independent, x.name, y.name){
  # Presentation
  t.name <- paste("BoxPlot of", x.name, "vs.", y.name)
  
  t.table <- table(independent, dependent)
  
  mosaicplot(t.table, main=t.name, shade=TRUE)
  
  k <- chisq.test(t.table)
  
  association <- k$p.value < 0.01
  
  return(association)
}


f.examine.ordinal <- function(dependent, independent, x.name, y.name){
  # Presentation
  t.name <- paste("BoxPlot of", x.name, "vs.", y.name)
  
  boxplot(independent~dependent, main = t.name, col = "red")
  
  k <- kruskal.test(independent~dependent)
  
  association <- k$p.value < 0.01
  
  return(association)
}

```

Now, we perform analysis on our dataset. 

```{r}
# Outcome Dataframe 
t <- data.frame(independent = character(),
                association = logical(),
                stringsAsFactors = FALSE)

# Dependent Variable
dependent <- data$`Outcome`
y <- "Outcome"

# Loop to Find Association(s)
for (x in names(data)[!names(data) %in% "Outcome"]) {
    independent <- data[[x]]
    
    if (is.numeric(independent)) {
      outcome <- f.examine.numeric(dependent, independent, x, y)
    } else {
      if (is.ordered(independent)) {
        outcome <- f.examine.ordinal(dependent, independent, x, y)
      } else {
        outcome <- f.examine.categorical(dependent, independent, x, y)
      }
    }
    
    t <- rbind(t, data.frame(independent = x, association = outcome))
}

# Filter No Association
t %>% filter(!association)

```

We find 4 variables that were found to have no association with the outcome variable: gender, jundice, used_app_before, and relation. 

Looking through the graphical representations of these relationships, it is interesting to note that the A1-A10 scores were showing clear association. 

This concludes the analysis and exploration portion of our dataset. We now move to prepare our dataset for training a few models. 

### Data Preparation

#### Data Split

```{r}
# Set Seed
set.seed(1)

# Test Split
train.sample <- createDataPartition(data$`Outcome`, p=0.9, list=FALSE)
train.data <- data[train.sample, ]
test.data <- data[-train.sample, ]

# Labels 
test.data.labels <- test.data$'Outcome'
```

Additionally, since some of our future models will require us to have a separate validation set (namely, neural networks) - we perform as such below. 

In order to not mess with the original splits (once we add imputed values), we also set the seed again. Note: we will use the CNN training set to compute imputations to avoid any possible leakage.  

```{r}
set.seed(1)

# Validation Split (CNN)
cnn.sample <- createDataPartition(train.data$`Outcome`, p=0.9, list=FALSE)

cnn.train <- train.data[cnn.sample, ]
cnn.validation <- train.data[-cnn.sample, ]

cnn.test <- test.data
cnn.test.labels <- test.data.labels

```

Further setup will be required for the CNN datasets before they are digestable for our neural network models. This will be performed ahead after further refinement. 

#### Imputation

As we examined in the data exploration portion of this report, we find three main variables with missing values: age, ethnicity, and relation. 

With age missing only 0.28% of its total values, this points to its missing values being by chance rather than of that of systemic reason. For this reason, we will simply use the mode of our previously defined age buckets to impute the missing values using the training data, below, in order to avoid any spill.


```{r}
# Mode Helper - from past assignments 
f.mode <- function(x) {
  t <- table(x)
  mode_values <- names(t)[t == max(t)]
  return(mode_values[1])  
}

# Impute Age
data$age[is.na(data$age)] <- f.mode(cnn.train$age)
```

As discussed in the exploration phase of our project, we found that the ethnicity and relation variable were not only both missing the same number of entries, but were also missing the same entries in addition to missing more than 13% of the total entries in the dataset. 

For these reasons, we will take a different approach - due to the sheer number of missing entries, we will create an additional category, unknown. 
Additionally, due to both variables missing the same entries, we will create an additional variable, missing_background, to indicate if the candidate was missing these values. 

```{r}
# Create indicator
data$missing_background <- is.na(data$relation) & is.na(data$ethnicity)

# Store levels
t.ethnicity <- c(levels(data$ethnicity), "Unknown")
t.relation  <- c(levels(data$relation), "Unknown")

# Add Updated Levels
data$ethnicity <- factor(data$ethnicity, levels = t.ethnicity)
data$relation  <- factor(data$relation, levels = t.relation)

# Impute
data$ethnicity[data$missing_background] <- "Unknown"
data$relation[data$missing_background]  <- "Unknown"
data$missing_background <- as.integer(data$missing_background)


colSums(is.na(data))
```

We find that we have successfully handled all missing values on our dataset. 

We move to resplit our data, to properly ensure that none of our datasets have any missing values.

```{r}
# Set Seed
set.seed(1)

# Test Split
train.sample <- createDataPartition(data$`Outcome`, p=0.9, list=FALSE)
train.data <- data[train.sample, ]
test.data <- data[-train.sample, ]

# Labels 
train.data.labels <- train.data$`Outcome`
test.data.labels <- test.data$`Outcome`

# Prune Outcome Variable
test.data <- test.data %>%
  select(-`Outcome`)

## CNN Split 
set.seed(1)

# Validation Split (CNN)
cnn.sample <- createDataPartition(train.data$`Outcome`, p=0.9, list=FALSE)

cnn.train <- train.data[cnn.sample, ]
cnn.train.labels <- cnn.train$`Outcome`

cnn.validation <- train.data[-cnn.sample, ]
cnn.validation.labels <- cnn.validation$`Outcome`

cnn.test <- test.data
cnn.test.labels <- test.data.labels

# Prune
cnn.train <- cnn.train %>%
  select(-`Outcome`)
cnn.validation <- cnn.validation %>%
  select(-`Outcome`)

```

Before continuing, we run a quality control check to see how our data is structure to ensure it meets a completed checklist of factors and there aren't any weird behaviors from our earlier ventures. 

```{r}
str(data)
```

## Heuristic Benchmark 
Performed by J. Carlos Garcia

For a candidate to be referred for further evaluation for autism spectrum disorder, they must score a higher than 7 on the screening questionnaire. 

As a result, we will use the result feature as our baseline and assume that any candidate with a result higher than 7 has a positive outcome. We will check the performance of this benchmark to compare to in the future for our models. 

```{r}
# Assign High Risks
risk.train.score <- ifelse(train.data$result >= 7, "High", "Low")
risk.test.score  <- ifelse(test.data$result  >= 7, "High", "Low")
```

Using our risk scores, high risk or low risk based on the result feature, we produce our "model":

```{r}
t <- train.data.labels

# Mean ASD Rate
model.risk <- train.data %>%
  filter(risk.train.score == "High") %>%
  summarise(t.prob = mean(t == "YES"))

model.risk

```

We attain a value of 26.5% for an ASD rate on high risk candidates. 

```{r}
# Map Probability
set.seed(1)
p.risk.prediction <- 
  ifelse(risk.test.score == "High",
         rbinom(length(risk.test.score), 1, 0.269),0)

risk.prediction <- ifelse(p.risk.prediction == 1, "YES", "NO")


# Outcomes 
risk.crosstab <- table(Predicted=risk.prediction,Actual=test.data.labels)

risk.crosstab
```

Taking our 26.9% of a HIGH risk having a positive outcome, we derrive the following (it is worth noting that our benchmark did not score a single false positive)

Our benchmark achieves:

```{r}
# Counts
risk.TP <- 3
risk.FP <- 0
risk.FN <- 15
risk.TN <- 51

# Performance
risk.recall <- risk.TP / (risk.TP + risk.FN)
risk.precision <- risk.TP / (risk.TP + risk.FP)
risk.f1 <- 2 * ((risk.precision * risk.recall) / (risk.precision + risk.recall))
risk.accuracy <- (risk.TP + risk.TN) / (risk.TP + risk.FP + risk.FN + risk.TN)

# Output
cat("\nHeuristic Benchmark Performance\n")
cat("Precision:", round(risk.precision, 4), "\n")
cat("Recall:", round(risk.recall, 4), "\n")
cat("F1 Score:", round(risk.f1, 4), "\n")
cat("Accuracy:", round(risk.accuracy, 4), "\n")

```

Despite the benchmark model seemingly performing well on the crosstable, we find that it disproportionately picked NO. Our benchmark is very biased and is likely to label a candidate with ASD as NO. 

## Simple Models 
Performed by J. Carlos Garcia

#### Regularized Linear Regression Models

While our CNN models rely on a dedicated validation set, the models in this section will instead use cross-validation via the caret package. This approach allows our models to access as much training data as possible while still providing reliable performance estimates.

We apply 5-fold cross-validation, as it provides a good balance between minimizing overfitting and preserving sufficient training data per fold. This setup also ensures consistency when comparing performance across models.

**LASSO**

We begin with a LASSO regularization method, linear regression model. 

```{r}
set.seed(1)

lasso.ctrl <- trainControl("cv", number=5)
lasso.grid <- expand.grid(alpha=1,lambda=10^seq(-3, 3, length=100))

model.lasso <- caret::train(Outcome ~ .,
                            data = train.data,
                            method = "glmnet",
                            trControl = lasso.ctrl,
                            tuneGrid = lasso.grid)

```

Above, we trained our linear regression model using the LASSO regularization method. LASSO regularization performs variable selection and regularization of the model's coefficents which improves generalization (in theory) and interpretability by shrinking or eliminating less important features.

Our model, as will our other models below, use auto tuning of its parameters. For a linear regression model, we are tuning the regularization strength parameter, lambda, which controls the degree to which large coefficients are penalized, encouraging sparsity and simpler models

Additionally, as explained, we use the caret package to perform cross-validation folding which splits the data into equal subsets based on the specified number of folds parameter. This allows for multiple models to be trained, returning the optimal model (variance effect), paired with our LASSO regularization, resulting in a lambda value that minimizes classification error across validation folds. 

With this in mind, we find that LASSO produces the following:

```{r}
lasso.coef <- coef(model.lasso$finalModel, model.lasso$bestTune$lambda)
lasso.coef
```

We find that for our LASSO linear regression model, a single variable — result — explains nearly all the variance in predicting ASD outcomes. In other words, LASSO identified result, the pre-screening questionnaire score, as the most important feature for determining whether a candidate receives a positive ASD diagnosis.

With a predicted probability of 95.5%, the model suggests that a candidate with a sufficiently high result score is highly likely to receive a positive outcome. This reinforces the pivotal role of the pre-screening survey in the overall diagnostic journey.


**RIDGE**

Before we train our RIDGE Linear Regression Model, we note a distinction between the two models. 

Unlike LASSO, Ridge regularization does not have the ability to eliminate variables from the model. 

While LASSO can shrink some coefficients all the way to zero — effectively removing less important features — Ridge instead shrinks all coefficients toward zero, but retains all variables in the model. This makes Ridge particularly effective in settings where many predictors contribute small but meaningful effects.

```{r}
set.seed(1)

ridge.ctrl <- trainControl("cv", number=5)
ridge.grid <- expand.grid(alpha=0,lambda=10^seq(-3, 3, length=100))

model.ridge <- caret::train(Outcome ~ .,
                            data = train.data,
                            method = "glmnet",
                            trControl = ridge.ctrl,
                            tuneGrid = ridge.grid)

```

RIDGE operates similarly to our LASSO model in terms of overall workflow: the goal is to fine-tune the lambda parameter to strike a balance between model complexity and classification performance. However, unlike LASSO, 

As mentioned, RIDGE does not eliminate variables; instead, it shrinks all coefficients, preserving the contribution of each feature while reducing their overall magnitude.


```{r}
ridge.coef <- coef(model.ridge$finalModel, model.ridge$bestTune$lambda)
ridge.coef
```

As we see above, the RIDGE model preserves all variables, of note - it assigns a coefficent of 0.35 to result, a massive difference betwene our two models. This difference offers insight to the functional differences between the two regularization methods. 

**Elastic Net**

Elastic Net (enet) combines the penalties of both LASSO (L1) and RIDGE (L2). It introduces an additional variable, alpha, which controls the balance between L1 and L2, with lambda holding its traditional regularization strength intention. 

The alpha balance between L1 and L2 is as follows:

alpha=1, regularization is performed by LASSO 
alpha=0, regularization is performed by RIDGE
0 < alpha < 1, regularization is performed by both. 

We train our enet model below, using the same auto tuning methods and cross-validation function as for the other two models. 

```{r}
set.seed(1)

enet.ctrl <- trainControl("cv", number=5)
enet.grid <- expand.grid(alpha=seq(0,1,length=10)
                         ,lambda=10^seq(-3, 3, length=100))

model.enet <- caret::train(Outcome ~ .,
                            data = train.data,
                            method = "glmnet",
                            trControl = enet.ctrl,
                            tuneGrid = enet.grid)

```

Our model produces the following coefficients:

```{r}
cat("\nWith an alpha of ", model.enet$bestTune$alpha, "we attain the following coefficients\n\n")

enet.coef <- coef(model.enet$finalModel, model.enet$bestTune$lambda)
enet.coef
```

With an alpha value of 1, we find that enet gave full control to the LASSO regularization method, resulting in the same coefficient and weight on result. This also implies that the LASSO method outperforms the RIDGE method on validation data. 

#### Regularized Forest Models

TODO: WRITEUP

**Random Forest Model**

TODO: WRITEUP

```{r}
set.seed(1)

rf.ctrl <- trainControl("cv", number=5)
rf.grid <- expand.grid(mtry=c(2, 4, 8, 16))

model.rf <- caret::train(Outcome ~ .,
                         data = train.data,
                         method = "rf",
                         metric="Kappa",
                         trControl = rf.ctrl,
                         tuneGrid = rf.grid,
                         importance=T
                         )


```

TODO: WRITEUP

```{r}
rf.varimp <- varImp(model.rf)
rf.varimp
```

TODO: WRITEUP

**Gradient Boosted Tree Model**

TODO: WRITEUP

```{r}
set.seed(1)

gbm.ctrl <- trainControl("cv", number=5)

model.gbm <- caret::train(Outcome ~ .,
                         data = train.data,
                         method = "gbm",
                         trControl = gbm.ctrl,
                         verbose=FALSE
                         )


```

TODO: WRITEUP

```{r}
gbm.varimp <- varImp(model.gbm)
gbm.varimp
```

#### Support Vector Machines

TODO: WRITEUP

**Linear SVM**

TODO: WRITEUP

```{r}
set.seed(1)

# the same for both
svm.ctrl <- trainControl("cv", number=5)

model.svm.linear <- caret::train(Outcome ~ .,
                         data = train.data,
                         method = "svmLinear",
                         trControl = svm.ctrl, preProcess=c("center", "scale")
                         )


```

TODO: WRITEUP

**Linear SVM**

TODO: WRITEUP

```{r}
set.seed(1)

# the same for both
svm.ctrl <- trainControl("cv", number=5)

model.svm.radial <- caret::train(Outcome ~ .,
                         data = train.data,
                         method = "svmRadial",
                         trControl = svm.ctrl, preProcess=c("center", "scale")
                         )


```

TODO: WRITEUP

#### Simple Model Comparison

```{r}
t <- list(lasso=model.lasso, 
          ridge=model.ridge, 
          elastic=model.enet, 
          rf=model.rf,
          gbm=model.gbm,
          svmlinear=model.svm.linear,
          svmradial=model.svm.radial
          )

model.compare <- caret::resamples(t)

summary(model.compare)
```

TODO: WRITEUP

## Complex Models 
Performed by 

## Performance
Performed by 

## Ethics
Performed by 
